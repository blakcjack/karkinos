---
title: "Titanic Life Prediction"
author: "Suberlin Sinaga"
header-includes: |
    \usepackage[familydefault,regular]{Chivo} %% Option 'familydefault'
    \usepackage{xcolor}
format:
  pdf:
    highlight-style: dracula
mainfont: SourceSansPro-Regular
editor_options: 
  chunk_output_type: inline
output:
  pdf_document:
    toc: true
    toc_float: true
    number_sections: true
    extra_dependencies: ["flafter"]
filters: 
    - color-text.lua
---

```{=tex}
\definecolor{primary}{RGB}{0,63,160}
\definecolor{title}{RGB}{0,95,0}
\definecolor{secondary}{RGB}{0,169,188}
\definecolor{info}{RGB}{82,197,58}
\definecolor{accent1}{RGB}{203,4,229}
\definecolor{accent2}{RGB}{77,207,212}
\definecolor{accent3}{RGB}{182,223,211}
\definecolor{good}{RGB}{2,213,85}
\definecolor{warning}{RGB}{254,211,2}
\definecolor{danger}{RGB}{255,45,74}
```
```{r setup_chunk, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# importing library
suppressPackageStartupMessages({
    library(dplyr)
    library(tidymodels)
    library(ggpubr)
    library(tidyquant)
    library(C50)
    library(rules)
    library(vip)
    library(bonsai)
})

# Theming section
used_theme <- theme_tq(base_size = 12) +
    theme(plot.background = element_rect(fill = "#C7E5C2"),
          panel.background = element_rect(fill = "#bddbb8"),
          strip.background = element_rect(fill = "#1A1A1A"),
          text = element_text(colour = "#333333", face = "bold"),
          axis.text = element_text(colour = "#333333"),
          strip.text = element_text(colour = "#bddbb8"),
          legend.box.background = element_rect(fill = "#bddbb8"),
          legend.background = element_rect(fill = "#bddbb8"),
          legend.key = element_rect(fill = "#bddbb8"),
          legend.position = "bottom",
          plot.caption = element_text(color = "#333333", size = 8),
          plot.title = element_text(color = "#005f00"),
          axis.title = element_text(color = "#005f00"),
          panel.grid.minor = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.grid.major.y = element_line(colour = "#96b392", linewidth = 0.01),
          plot.subtitle = element_text(size = 10)
    )

theme_set(used_theme)

survived_color <- c("0" = "#CD5c5c", "1" = "#228B22")

# set up some function to avoid load the whole package
`%<>%` <- magrittr::`%<>%`
set.seed(453)

answer_key <- read.csv("C:/Users/adika/OneDrive/Desktop/temp/titanic.csv")
```

## [A. Background and Problem Statement]{color="title"}

The Titanic ship sank years ago. Some of the passengers are survived from the disaster while the rest can't make it. The data indicates that the Titanic's passenger survival condition follow some specific pattern. The objective is to predict the survivalability of a passenger by considering known factors into account.

## [B. Data Understanding]{color="title"}

The data used comes from famous [kaggle titanic](https://www.kaggle.com/competitions/titanic) problem set.

```{r importing_data, echo=TRUE}
# here::here()
titanic_training_set <- read.csv(here::here("data/titanic/train.csv"))
titanic_test_set <- read.csv(here::here("data/titanic/test.csv"))
```

Since in this early dataset the training and testing set was separated, I will combine them to ensure that I understand all the data as a whole.

```{r combining_data_set, echo=TRUE}
titanic_raw_all <- rbind(
    titanic_test_set %>% mutate(Survived = NA, Source = "test set"),
    titanic_training_set %>% mutate(Source = "train set")
)

y_var <- titanic_training_set$Survived
```

Naturally, the testing set has no target variable, hence I set the target variable values into `NA` when the data came from testing set.

```{r}
titanic_raw_all %>% str()
```

As shown, the dataset has 12 variables (exclude the Source variable). Between the 12 variables, 1 of them is dependent variable while the rest are independent.

Another thing that I find out is that the variables name started by capital letter here. Just for the standardization, I will use snake_case format for the variable name.

```{r}
titanic_raw_all %<>% janitor::clean_names()
```

Now, I will start to see overall summary of the data.

```{r}
a <- titanic_raw_all %>% DataExplorer::introduce()
with(a, {
    print(paste("Total Rows :", rows))
    print(paste("Total Columns :", columns - 1))
    print(paste("Total Observations :", (columns- 1) * rows))
    print(paste("Total Discrete Columns :", discrete_columns - 1))
    print(paste("Total Continuous Columns :", continuous_columns))
    print(paste("Total Missing Values :", total_missing_values))
})
```

## [C. Data Cleaning]{color="title"}

Before further using the data, I will first do some basic cleaning up. One of the basic clean up that I will apply is missing values handling.

## DATA EXPLORATION

### Data in Brief

```{r}
titanic_raw_all %>% str()
```

Here I have 13 variables, from PassengerId to Survived. The source variable is an addition to mark where the data comes from, so basically I have only 12 variables come from the data. First thing I notice here is that the name of the variable is not following `snake_case` format. Hence I will first standardized them.

```{r clean_up_names, echo=TRUE}
titanic_raw_all %<>% janitor::clean_names()

titanic_raw_all %>% str()
```

Now the columns have been all standardized. Now the next question is that, what is the first variable to explore. Since I would like to explore them in a set line. I will first check the variable with highest correlation to the target first.

```{r}
titanic_raw_all[complete.cases(titanic_raw_all),] %>% 
    select(-source, -cabin, -name, -ticket) %>% 
    mutate(sex = case_when(sex == "male" ~ 1, TRUE ~ 0),
           embarked = factor(embarked, levels = c("S", "Q", "C"))) %>%
    DataExplorer::plot_correlation()
```

This correlation plot is a very early correlation plot In this plot:

1.  I encode sex to use 1 and 0, where 1 for male and 0 for female.

2.  I remove column with too many unique values such as name, cabin, and ticket.

3.  I also remove rows contains missing values.

From the plot, I know that sex, pclass, fare, embarked, and age. Those 5 variables are highly correlated to the target and I will first investigate them.

### 1. Survived Variable

This variable is basically a binary variable where the value is either 0 or 1. It represents whether the passenger will survive from the disaster or not. Considering the data, it should be stored as factor but yet the data was stored in `integer` data type.

```{r}
titanic_training_set %>% 
    mutate(Survived = as.factor(Survived)) %>% 
    ggplot(aes(x = Survived, y = after_stat(count))) +
    geom_bar(aes(fill = Survived), width = 0.5) +
    geom_label(aes(label = after_stat(count)), stat = "count", vjust = 1) +
    scale_fill_manual(values = survived_color)
```

The graph shows that people who survive on the training data is 342 (38.4%) and the one that is not survived is 549 (61.6%). The ratio between them is 1 : 1.6 which can be consideered as balance dataset.

### 2. Sex Variable

Sex variable is defining the gender of the passenger. Basically, this variable is also have to set to factor, but the default type of the variable is character or string.

```{r}
titanic_raw_all %>% 
  filter(source == "train set") %>% 
  count(sex) %>% 
  ggplot(aes(x = sex, y = n)) +
  geom_bar(stat = "identity", width = 0.5, fill = "#228B22") +
  geom_label(aes(label = n), vjust = 1, fill = "#bddbb8")
```

It seems that male passengers are more common on Titanic than female passengers. Now the main question for me is how gender actually related to survival rate of the passengers.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    count(sex, survived) %>% 
    group_by(sex) %>% 
    mutate(survived = as.factor(survived),
           prop = n/sum(n)) %>% 
    ggplot(aes(x = sex)) +
    geom_bar(aes(y = prop, fill = survived), stat = "identity", position = "dodge", width = 0.5) +
    scale_fill_manual(values = survived_color) +
    geom_label(aes(y = prop, label = percent_format()(prop)), hjust = c(1.3,-0.3,1.3,-0.3))
```

The plot indicates very interesting fact that stated that by being a female, you can have about 74.2% of survive probability.

### 3. Pclass Variable

Pclass variable defines and describe about the social class of a person. The higher the pclass, the higher the class.

```{r}
titanic_raw_all %>% 
    mutate(pclass = factor(pclass, level = c(1,2,3))) %>%
    ggplot(aes(x = pclass)) +
    geom_bar(fill = "#228B22", width = .5) +
    geom_label(aes(label = after_stat(count)), stat = "count",
               vjust = 1, width = 0.5, fill = "#bddbb8")
```

There are three types of the class, represented by number 1 to 3. Number 1 represents the highest class, and 3 represents the lowest class. Just like an economic class, pclass number 3 has the largest proportion.

In theory, the higher the class of the person, the higher the survival chance of them due to exclusivity and privilege. Take a look at the following graph.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>%  
    count(pclass, survived) %>% 
    group_by(pclass) %>%
    mutate(pclass = factor(pclass, level = c(1,2,3)),
           prop = n/sum(n),
           survived = as.factor(survived)) %>% 
    ggplot(aes(x = pclass, y = prop)) +
    geom_bar(aes(fill = survived), position = "dodge", stat = "identity", width = 0.5) +
    geom_label(aes(label = percent_format()(prop)), hjust = rep(c(1, -0),3)) +
    scale_fill_manual(values = survived_color)
```

As expected, the higher the pclass, the better their survival live. The next question is how pclass is related to sex variable?

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>%  
    count(sex, pclass, survived) %>% 
    group_by(sex, pclass) %>%
    mutate(pclass = factor(pclass, level = c(1,2,3)),
           prop = n/sum(n),
           survived = as.factor(survived)) %>% 
    ggplot(aes(x = pclass, y = prop)) +
    geom_bar(aes(fill = survived), position = "dodge", stat = "identity", width = 0.5) +
    geom_label(aes(label = percent_format()(prop)), hjust = rep(c(1, -0),6),
               size = 3) +
    facet_grid(. ~ sex) +
    scale_fill_manual(values = survived_color)
```

I think from the trend wise, the chart indicates that pclass variable is independent towards gender. It has higher survival rate for pclass 1 and lowest survival rate for pclass 3 no matter what the gender is.

### 4. Fare Variable

Fare variable defines how much Titanic's passengers spend to get onboarded into Titanic ship. Since fare variable is a continuous variable, I will start exploring them using histogram.

```{r}
titanic_raw_all %>%
    ggplot(aes(x = fare)) +
    geom_histogram(fill = survived_color['1']) +
    scale_x_continuous(breaks = c(seq(1, 100, 20),seq(100, 500, 100)))
```

The fare variable is very right skewed. I also Found some outliers in the data.

```{r}
titanic_raw_all %>%
    filter(!is.na(survived)) %>% 
    ggplot(aes(x = fare)) +
    # geom_histogram(aes(y = ..density.., fill = as.factor(survived)), alpha = 0.3) +
    geom_density(aes(fill = as.factor(survived)), alpha = 0.3, col = NA) +
    scale_x_continuous(breaks = seq(0, 500, 30)) +
    # facet_wrap(survived ~ ., scales = "free_x") +
    scale_fill_manual(values = survived_color)
```

The graph indicates that at a glance, the survival and not survival passengers have different distribution plot. The graph said that the higher the fare, the higher the survival rate will be. The graph also indicates that I have outliers in the data. To overcome this outliers, I think I will grouping the data into bucket. The question is, what is the bucket that we can use?

```{r}
titanic_raw_all %>%
    summarize(fare_25 = quantile(fare, 0.25, na.rm = TRUE),
              fare_50 = quantile(fare, 0.5, na.rm = TRUE),
              fare_75 = quantile(fare, 0.75, na.rm = TRUE))
```

I will try splitting the data based on their quantile values.

```{r}
titanic_raw_all %>%
    filter(!is.na(survived)) %>% 
    mutate(fare_category = factor(case_when(between(fare, 0, 8) ~ "(0, 8)",
                                            between(fare, 9, 14) ~ "[8, 14)",
                                            between(fare, 15, 31) ~ "[14, 31)",
                                     fare > 31 ~ "(31,]"),
                                  levels = c("(0, 8)", "[8, 14)", "[14, 31)", "(31,]"))) %>% 
    ggplot(aes(x = fare_category)) + 
    geom_bar(aes(fill = as.factor(survived)), position = "dodge", width = 0.5) +
    scale_fill_manual(values = survived_color)
```

Based on this bucketing, the low fare data (between 0 and 8) is mostly filled by not survived passengers. While the highest fare bucket (\> 31) has higher survival rate.

### 5. Embarked Variable

Embarked variable talks about the port where the passengers are embarked from.

```{r}
titanic_raw_all %>% 
    ggplot(aes(x = forcats::fct_infreq(embarked))) +
    geom_bar(fill = survived_color['1'], width = 0.5) +
    geom_label(aes(label = after_stat(count)), stat = "count", vjust = 0,
               fill = "#bddbb8")
```

Now, the next question is, how is the survival rate for each of the category.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    ggplot(aes(x = forcats::fct_infreq(embarked))) +
    geom_bar(aes(fill = as.factor(survived)), position = "dodge", width = 0.5) +
    scale_fill_manual(values = survived_color)
```

The chart shows that people who gets embarked from S port has lower survival rate. But the question is, how can this embarked port affects survival rate?

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    ggplot(aes(x = forcats::fct_infreq(embarked))) +
    geom_bar(aes(fill = as.factor(survived)), position = "dodge", width = 0.5) +
    geom_label(aes(label = after_stat(count)), stat = "count",
               fill = "#bddbb8") +
    scale_fill_manual(values = survived_color) +
    facet_wrap(. ~ pclass)
```

So, basically people who embarked from Southampton port are mostly people with pclass 3. Hence the survival rate is very low. While people embarked from port Queenstown are mostly people with highest pclass.

### 6. Age Variable

Age variable stores information about how old the passenger is. Since this is a numeric variable, I will start with histogram.

```{r}
#| warning: false
#| message: false
titanic_raw_all %>% 
    ggplot(aes(x = age)) +
    geom_histogram(fill = survived_color['1'])
```

The histogram plot indicates that the data is right skewed with some outliers. Before dealing with the outliers, I will check the density distribution first.

```{r}
titanic_raw_all %>% 
    filter(source == "train set") %>% 
    ggplot(aes(x = age)) +
    geom_density(aes(fill = as.factor(survived)), width = 0.5, alpha = 0.4) +
    scale_fill_manual(values = survived_color) +
    scale_x_continuous(breaks = seq(0, 80, 5))
```

The density distribution says that

```{r}
titanic_raw_all %>% 
    filter(source == "train set") %>% 
    ggplot(aes(x = age)) +
    geom_density(aes(fill = as.factor(survived)), alpha = 0.4) +
    scale_fill_manual(values = survived_color) +
    scale_x_continuous(breaks = seq(0, 80, 5)) +
    facet_grid(. ~ sex, scales = "free")
```

AS for the age, I think I will group them into bucket to see if each bucket has different pattern.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(age_category = factor(case_when(between(round(age), 0, 14) ~ "children",
                                           between(round(age), 15, 24) ~ "teenager",
                                           round(age) > 24 ~ "adult"),
                                           levels = c("children", "teenager",
                                                      "adult"))) %>% 
    ggplot(aes(x = age_category)) + 
    geom_bar(aes(fill = as.factor(survived)), position = "dodge", width = 0.5) +
    scale_fill_manual(values = survived_color)
```

Based on the chart, I think, only children who has better survival rate. Now, I will check the pattern accross gender.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(age_category = factor(case_when(between(round(age), 0, 14) ~ "children",
                                           between(round(age), 15, 24) ~ "teenager",
                                           round(age) > 24 ~ "adult"),
                                           levels = c("children", "teenager",
                                                      "adult"))) %>% 
    ggplot(aes(x = age_category)) + 
    geom_bar(aes(fill = as.factor(survived)), position = "dodge", width = 0.5) +
    facet_wrap(. ~ sex) +
    scale_fill_manual(values = survived_color)
```

### 7. Passenger ID variable

This variable is basically just a random value assigned to the passenger. It purposes is to easily identify the passenger uniquely, hence I will just [drop]{color="danger"} them.

### 8. Name Variable

In simple, name is something that everybody has. It is attached to a person but not unique for each person. I and you might have the same name. So basically, it just a random values attached to us to identify ourself and nothing to do with survival life. Naturally it will and should be deleted. But in this titanic data, name comes with a unique thing that further can be used to identify socio economic class, such as Mr, Mrs, Jonkheer, etc.

```{r}
name_decoder <- function(x) {
    x = as.character(x)
    ses_name = sapply(x, function(y) { strsplit(y, ", ")[[1]][2]})
    identifier_name = sapply(ses_name, function(x) {strsplit(x, ".", fixed = TRUE)[[1]][1]})
    return(identifier_name)
}
titanic_raw_all %>% 
    mutate(names = name_decoder(name)) %>% 
    ggplot(aes(x = forcats::fct_infreq(names))) +
    geom_bar(fill = survived_color["1"], width = .5) +
    theme(axis.text.x = element_text(angle = 90)) +
    labs(title = "Distribution of SES in Titanic Data",
         x = "SES Based On Name",
         y = "Countc")
```

The SES in names are dominated by 4 big categories. Since there too many categories with low data point, I think they can be lumped into 'other'.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(names = name_decoder(name),
           survived = as.factor(survived)) %>% 
    mutate(names = case_when(names %in% c("Mr", "Miss", "Mrs", "Master") ~ names, TRUE ~ "other")) %>% 
    ggplot(aes(x = forcats::fct_infreq(names))) +
    geom_bar(aes(fill = survived), position = "dodge", width = .5) +
    theme(axis.text.x = element_text(angle = 90)) +
    scale_fill_manual(values = survived_color)
```

As expected, the people with "Mr" has low survival rate compared to Master. It indicates that grown man has lower survival probability than the younger one. The interesting fact is that Miss as not yet married female (can be considered as younger) has lower survival probability than the married one.

In fact, Mr, Mrs, Miss, and Master are very related to gender. So we can say that those SES is closely related to gender. Considering this thing, I will take a look at the gender.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(names = name_decoder(name),
           survived = as.factor(survived)) %>% 
    mutate(names = case_when(names %in% c("Mr", "Miss", "Mrs", "Master") ~ names, TRUE ~ "other")) %>% 
    ggplot(aes(x = forcats::fct_infreq(names))) +
    geom_bar(aes(fill = survived), position = "dodge", width = .5) +
    theme(axis.text.x = element_text(angle = 90)) +
    scale_fill_manual(values = survived_color) +
    facet_wrap(. ~ sex, scales = "free")
```

The data based on gender said that Other category on female are all survive, while on male categories, almost all type of SES name have lower survival rate but other.

### 9. Sibling and spouse Variable

Sibling and spouse variable is represented by sib_sp. It is talking about how many siblings and or spouse brought into the ship.

```{r}
titanic_raw_all %>%
    filter(!is.na(survived)) %>% 
    mutate(survived = as.factor(survived)) %>% 
    ggplot(aes(x = sib_sp)) +
    geom_bar(aes(fill = survived), width = .5 ,position = "dodge") +
    scale_x_continuous(breaks = 0:8) +
    scale_fill_manual(values = survived_color)
```

It seems that the probability of survive will greatly increase if you have only 1 siblings or spouse. The probability seems to drop either you don't have siblings or spouse or you have more siblings or spouse. I think, this condition is very closely related to gender. In fact for example, if you are a woman and you have husband, you will be more likely to survive. I also need to confirm whether the people with 0 sib_sp and survive are dominated by male or not.

```{r}
titanic_raw_all %>%
    filter(!is.na(survived)) %>% 
    mutate(survived = as.factor(survived)) %>% 
    ggplot(aes(x = sib_sp)) +
    geom_bar(aes(fill = survived), width = .5, position = "dodge") +
    scale_x_continuous(breaks = 0:8) +
    facet_grid(sex ~ ., scales = "free_y") +
    scale_fill_manual(values = survived_color)
```

The chart indicated that for male, no matter how many siblings and spouse you have, you will have low survival rate. While for female, having more siblings seem a problem.

### 10. Parch Variable

Parch variable talks about parents and or children get onboarded into the ship.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(survived = as.factor(survived)) %>%
    ggplot(aes(x = parch)) +
    geom_bar(aes(fill = survived), width = .5, position = "dodge") +
    scale_x_continuous(breaks = 0:8) +
    scale_fill_manual(values = survived_color)
```

I think parch variable has the same pattern with sib_sp variable, the more children you have, the lower your survival rate will be. The next question is about the trend across gender. Are they also share same pattern?

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(survived = as.factor(survived)) %>%
    ggplot(aes(x = parch)) +
    geom_bar(aes(fill = survived), width = .5, position = "dodge") +
    scale_x_continuous(breaks = 0:8) +
    facet_grid(sex ~ ., scales = "free_y") +
    scale_fill_manual(values = survived_color)
```

Since both data represents same pattern even across gender, they are naturally highly correlated. Take a look at the following chart.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>%
    select(sib_sp, parch, survived) %>% 
    DataExplorer::plot_correlation(title = "Correlation Between Survival Rate, Parch, and Sib_sp Variables")
```

As seen that parch and sib_sp is 41% positively correlated, and only has 0.08, and -0.04 correlation with survival rate. This gives me an idea. What if I merged both of them into a single category to explain the family size.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>%
    ggplot(aes(x = as.numeric(parch) + as.numeric(sib_sp) + 1)) +
    geom_bar(aes(fill = as.factor(survived)), width = .5, position = "fill") +
    scale_x_continuous(breaks = 0:11) +
    scale_fill_manual(values = survived_color) +
    facet_grid(sex ~ ., scales = "free")
```

I think, the chart still gives me same pattern but now in more robust information. Based on the family size, I can create new grouping variable I will called as family category. To divide them into bucket or group, I will use the following rules:

-   If the number of family size is 1, then it will be categorized as `no family`.

-   If the number of family size is between 2 and 3, then it will be categorized as small family

-   If the number of family size is 4, then it will be categorized as medium family

-   If the number of family size is greater than 4, then it will be categorized as big family.

### 11. Ticket Variable

The ticket variable contains a random variable. In fact, in this data, I found out that some people have equal ticket values.

```{r}
titanic_raw_all %>% 
    count(ticket) %>% 
    top_n(10, n) %>% 
    arrange(desc(n))
```

I am afraid that there is correlation between the number of ticket and the survival rate.

```{r}
titanic_raw_all %>% 
    left_join(titanic_raw_all %>% count(ticket) %>% filter(ticket != "", !is.na(ticket)), by = "ticket") %>%
    filter(!is.na(survived)) %>% 
    mutate(survived = as.factor(survived)) %>% 
    ggplot(aes(x = n)) +
    geom_bar(aes(fill = survived), position = "dodge") +
    scale_x_continuous(breaks = 1:11)
```

It seems that people who shares 2,3 and 4 same tickets has higher survival rate. I think, I will insert this feature into my raw data.

```{r}
titanic_raw_all %<>%
    left_join(titanic_raw_all %>% count(ticket) %>% filter(ticket != "", !is.na(ticket)), by = "ticket") %>% 
    rename(ticket_num = n)
```

### 12. Cabin

The cabin variable mostly talk about the cabin name/code where the passengers placed. But somehow, there are so many NAs in this data.

```{r}
titanic_raw_all %>% 
    mutate(cabin = case_when(cabin == "" ~ NA, TRUE ~ "has cabin")) %>% 
    count(survived,cabin)
```

There is about 1014 or 77% the data is missing. This is of course hard to interpret or guess the missing values. Hence, I think I will drop the variable.

## DATA MODELING

My data modelling will consist of

-   data preprocessing

-   feature engineering

-   model training

-   model evaluation

-   model selection

-   model hyper parameter tuning

-   model prediction

### Data Preprocessing

In this preprocessing data, I will use 4C frameworks (chopping, cleaning, completing, correcting) and tidy principal. I will use tidy model and utilize recipe to preprocess and model the data.

#### Data Chopping/Splitting

I will split the data into 3 types of data. The first one is the training data set that will consist of 70% of the training set data, while the rest of the 30% will be testing set to test the model. The third one is the validation set that will be submit to kaggle to measure the final result.

```{r echo=TRUE}
i_train <- initial_split(titanic_raw_all %>% filter(source == "train set") %>% select(-source), prop = 0.7)

train_set <- training(i_train)

cross_val <- mc_cv(train_set)
    
test_set <- testing(i_train)

val_set <- titanic_raw_all %>% 
    filter(source == "test set") %>% 
    select(-source)
```

I will use tidymodels paradigm to build my model, and defining my recipe using training data set. In the early setup, I will only add survived as outcome.

```{r}
#| echo: true
my_recipe <- recipe(train_set) %>% 
    update_role(survived, new_role = "outcome")
```

#### Cleaning

Due to their usage, some variables might not be used as the predictor or outcome. I will simply add them as ids type of variables.

```{r}
#| echo: true
my_recipe_clean <- my_recipe %>% 
    update_role(passenger_id, ticket, cabin, name, new_role = "ids")
```

#### Completing

Completing process is a step to filling out missing data. It is also known as handling missing values step.

```{r}
# correcting the age data format to integer
titanic_raw_all %>% 
    select(-c(source,survived)) %>% 
    DataExplorer::plot_missing(missing_only = TRUE, theme_config = used_theme)
```

Previously, there were 3 variables have missing values. But now, the plot stated that there are only 2 variables. The reason behind this is that the missing values on embarked variable is not missing values, instead it is an empty string (""). Hence, I need to convert it first to missing values in order to ensure that it is acknowledged as missing values.

```{r}
titanic_raw_all %>% 
    select(-c(source,survived)) %>% 
    mutate(embarked = if_else(embarked == "", NA, embarked)) %>% 
    DataExplorer::plot_missing(missing_only = TRUE, theme_config = used_theme)
```

Based on previous exploration, the missing values on embarked variable can be filled using mode, while age can be filled using mean, and fare variable can be filled using median.

```{r filling_missing_values, echo=TRUE}
recipe_complete <- my_recipe_clean %>% 
    step_mutate(embarked = if_else(embarked == "", NA, embarked)) %>% 
    step_impute_median(age) %>% 
    step_impute_mode(embarked) %>% 
    step_mutate(embarked = droplevels(as.factor(embarked))) %>% 
    step_impute_median(fare)
```

#### Correcting

This step is also known as handling outliers. In previous exploration, I found out that some data such as age and fare has some outliers. But based on the following information fare outliers seem natural data since the ticket ranging from 7 euros up to 870 euros.

#### Converting

Converting is meant to convert the data type into what it should be. Previously, I have found out that some variables need to be converted into certain types. For example, pclass needs to be converted into ordinal factor since it is stored as integer. Surived, embarked, name and sex variables should also be stored as factor.

```{r}
#| echo: true
recipe_convert <- recipe_complete %>% 
    step_num2factor(pclass, levels = c("1","2","3"), ordered = TRUE) %>% 
    step_mutate(survived = factor(survived, levels = c("0", "1")), skip = TRUE) %>% 
    step_string2factor(sex, embarked)
```

### Feature Engineering

#### Assigning All features

Since I am using recipe packages, there is one extra step needed to define the role of the whole features.

```{r}
base_recipe <- recipe_convert %>% 
    update_role(sex, pclass, age, sib_sp, parch, fare, embarked, ticket_num, new_role = "predictor")

base_recipe$term_info
```

#### Feature Extraction

Based on previous analysis, there are several features that I can extract from current features. They are:

-   Socio economic title from name

-   Traveling type that describes traveling condition for each person.

-   Fare category from fare value.

-   Family size from adding number of sibling, souse, parents ad children.

-   Age category

-   Ticket frequency

```{r}
#| echo: true
recipe_extract <- base_recipe %>% 
  step_mutate(ses_name = case_when(name_decoder(name) %in% c("Mr", "Miss", "Mrs", "Master") ~ name_decoder(name),
                                   TRUE ~ "Other") %>%
                  factor(levels = c("Mr", "Miss", "Mrs", "Master", "Other")),
              fare_category = factor(case_when(between(round(fare), 0, 8) ~ "1. < 7",
                                     between(round(fare), 9, 15) ~ "2. 8-15",
                                     between(round(fare), 16, 25) ~ "3. 16-25", 
                                     between(round(fare), 26, 35) ~ "4. 26-35", 
                                     between(round(fare), 36, 50) ~ "5. 36-50", 
                                     TRUE ~ "6. >= 50"), levels = c("1. < 7", "2. 8-15",
                                                                    "3. 16-25", "4. 26-35",
                                                                    "5. 36-50", "6. >= 50")),
              traveling_type = factor(case_when(age <= 10 & parch == 0 ~ "children w/o parents",
                                         age <= 10 & between(parch, 1, 2) ~ "children w/ parent",
                                         sib_sp == 0 & parch == 0 ~ "Single Trip",
                                         sib_sp > 0 | parch > 0 ~ "Family Trip"),
                                      levels = c("children w/o parents",
                                                 "children w/ parent",
                                                 "Single Trip",
                                                 "Family Trip")),
              family_size = coalesce(sib_sp, 0) + coalesce(parch, 0),
              age_category = factor(case_when(between(round(age), 0, 5) ~ "1. 0-5",
                           between(round(age), 6, 10) ~ "2. 6-10",
                           between(round(age), 11, 17) ~ "3. 11-17",
                           between(round(age), 18, 30) ~ "4. 18-30",
                           between(round(age), 31, 40) ~ "5. 31-40",
                           between(round(age), 41, 60) ~ "6. 41-60",
                           TRUE ~ "7. >60"), levels = c("1. 0-5", "2. 6-10",
                                                        "3. 11-17", "4. 18-30",
                                                        "5. 31-40", "6. 41-60",
                                                        "7. >60")),
              role = "predictor")

recipe_extract %<>%
    step_rm(sib_sp, parch)
```

#### Feature Selection

In machine learning model, there is a Jargon known as **"garbage in, garbage out"**. It means, if we feed our model with garbage (irrelevant and un-useful features) then we will get another garbage as the output. Hence, feature selection is very important.

# Feature Importance

Feature importance is one of feature selection method that using a model to asses how important a feature to the model. Assuming all the variables above were getting used. I will measure how important each variable to predict the target variable. I will use simple logistic regression to build the measurement.

There are several types of variable importance measurement that can be done in R, they are permutation based, variance based and shapley based. While for the metrics, we can use accuracy, f1 score, roc_auc, and so on.

```{r}
#| warning: false
predict_kern <- function(object, newdata = train) {kernlab::predict(object, newdata) %>% mutate_all(as.numeric) %>% .[[".pred_class"]]}
predict_kern_frame <- function(object, newdata = train) {kernlab::predict(object, newdata) %>% mutate_all(as.numeric) }

logit_engine <- logistic_reg(penalty = c("roc_auc", "accuracy", "f_meas"))

logit_wf <- workflow() %>%
  add_model(logit_engine) %>%
  add_recipe(recipe_extract)

logit_mdl <- logit_wf %>%
  fit_resamples(cross_val)

collect_metrics(logit_mdl)
```

Taking all the variables into account gives me about 86% area under the curve and 82% accuracy which is too low. Why I said this is too low? It is because by using only gender, I can predict the survival rate for male and female with 75% accuracy. Considering the whole variables, I just increase the accuracy about 7%. This is quite embarrassing. I assume that there is something wrong with the features I used. In order to prove this, I will use variable importance to check the importance of the features to the model.

```{r}
 #| warning: false
logit_mdl <- logit_wf %>%
  fit(train_set)

set.seed(453)
p_firm_method <- logit_mdl %>%
    extract_fit_parsnip() %>%
    vi(
        method = "firm",
        train = juice(recipe_extract %>% prep()) %>% mutate(survived = survived),
        target = "survived",
        metric = "roc_auc",
        pred_wrapper = predict_kern,
        event_level = "first",
        feature_names = c("sex","embarked","pclass","family_size","ses_name","age_category",
                          "fare_category", "traveling_type", "family_size", "ticket_num", 
                          "age", "fare")
    ) %>%
  ggplot(aes(x = reorder(Variable, desc(Importance)), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  # ggtitle(bquote(FeatureImportance^2))
  labs(title = expression(paste("Feature Importance using ", FIRM^1," Method")))

p_permute_method <- logit_mdl %>%
    extract_fit_parsnip() %>%
    vi(
        method = "permute",
        train = juice(recipe_extract %>% prep()) %>% mutate(survived = survived),
        target = "survived",
        metric = "roc_auc",
        pred_wrapper = predict_kern,
        # pred_wrapper = kernlab::pred,
        # event_level = "first",
        feature_names = c("sex","embarked","pclass","family_size","ses_name","age_category",
                          "fare_category", "traveling_type", "family_size", "ticket_num",
                          "age", "fare")
    ) %>%
  ggplot(aes(x = reorder(Variable, desc(Importance)), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = expression(paste("Feature Importance using ", PERMUTE^2," Method")))

ggarrange(p_firm_method, p_permute_method, ncol = 1, nrow = 2)
```

As expected, the sex and ses_name variables are the most important variables on both measurement. Another thing for sure, `family_num` have no importance for both measurements. This is expected. Since family_num is the total of addition between`sib_sp` and `parch` variable, hence they will naturally be highly correlated to each other. Now, the next question is, which feature should be used? More feature will explain more variability, but in fact it will make the model easier to be prone to overfit. Since family number is proven has no effect than sip_sp and parch, then I will just remove it from the model.

Another important thing from the variable importance here is that, in both measurement, `fare` and `embarked` are both on the last list with the lowest importance. For me, this is a sign that indicates that both of the variables are removable. Since, I have fare_category that represents fare, I think I will just drop the fare and use the fare category that indicates better importance.

With these findings, what I would like to do is getting the feature that maximizes the performance of gender, and remove the one that has no effect.

```{r}
#| echo: true
#| warning: false
# base_recipe <- recipe_extract
# 
# main_pred <- c("sex")
# 
# # assuming sex is the main predictor
# all_pred <- c("ses_name", "pclass", "age","fare_category", "family_size", "family_type", "fare", "embarked")
# 
# logit_engine <- logistic_reg()
# 
# # base model as benchmark
# mdl_data <- workflow() %>%
#     add_model(logit_engine) %>%
#     add_recipe(base_recipe %>% step_select(eval(main_pred), survived)) %>%
#     fit_resamples(cross_val) %>%
#     collect_metrics() %>%
#     mutate(n_features = 1,
#            feature_name = main_pred,
#            current_feature = "sex")
# 
# for(i in seq(length(all_pred))) {
#   print(paste("Tes model with", i+1, "features"))
#   for (feature in all_pred) {
#     curr_recipe <- base_recipe %>%
#       step_select(eval(feature), eval(main_pred), survived)
# 
#     set.seed(453)
#     mdl_data <- workflow() %>%
#       add_model(logit_engine) %>%
#       add_recipe(curr_recipe) %>%
#       fit_resamples(cross_val) %>%
#       collect_metrics() %>%
#       mutate(n_features = length(main_pred)+1,
#              feature_name = paste(c(paste(main_pred, collapse = ", "), feature), collapse = ", "),
#              current_feature = feature) %>%
#       bind_rows(mdl_data)
#   }
#   # select the base model as benchmark
#   base_model <- mdl_data %>%
#     filter(n_features == max(n_features) - 1,
#            feature_name == paste(main_pred, collapse = ", "))
# 
#   better_feature <- mdl_data %>%
#     filter(n_features == max(n_features) & ((.metric == "accuracy" & mean > (base_model %>% filter(.metric == "accuracy") %>% pull(mean))) |
#              (.metric == "roc_auc" & mean > (base_model %>% filter(.metric == "roc_auc") %>% pull(mean))))) %>%
#     distinct(current_feature, feature_name, mean, .keep_all = TRUE) %>% 
#     group_by(feature_name, current_feature) %>%
#     mutate(n = n()) %>%
#     filter(n == 2)
# 
#   if(nrow(better_feature) == 0) {
#     break
#   }
#   better_feature <- better_feature %>%
#     summarise(mean = mean(mean)) %>%
#     ungroup() %>%
#     filter(mean == max(mean))
# 
#   last_model <- mdl_data %>%
#     filter(feature_name == better_feature$feature_name)
# 
#   better_acc <- (base_model %>% filter(.metric == "accuracy") %>% pull(mean)) <=
#     (last_model %>% filter(.metric == "accuracy") %>% pull(mean))
# 
#   better_roc <- (base_model %>% filter(.metric == "roc_auc") %>% pull(mean)) <=
#     (last_model %>% filter(.metric == "roc_auc") %>% pull(mean))
# 
#   if (!better_acc | !better_roc) {
#     stop("No model generates better performance")
#   }
# 
#   main_pred <- c(main_pred, last_model$current_feature[[1]])
# 
#   all_pred <- all_pred[!is.element(all_pred, main_pred)]
#   print(paste("feature selected is ", better_feature$current_feature))
# }
```

Based on the feature selection that I have done, there are only 5 features that is proven to have affect to increase the predictability, they are `sex`, `ses_name`, `sib_sp`, `pclass`, and `parch`. Even though it is stated fare_category and age didn't give significant contribution, but as the logic and the data said it contributes to the model.

```{r}
# new_recipe <- my_recipe %>% 
#   step_rm(family_num, fare, embarked, ticket, cabin, name, ses_name)
```

#### Feature Transformation

In this case, I think there is no variable that needs to be transformed. So i will skip this process.

### Model Training & Evaluation

Previously, I have built the data understanding for each variables. Now, I will model the data to predict the survival rate. There are currently 9 variables that I will use as features. The question is, what kind of algorithm that should be used to train the data. There are several types of classification algorithm.

-   Logistic regression

-   Decision Tree

-   Random Forest

-   XGB (boosting tree)

-   GBM (boosting tree)

-   rpart (bagging tree)

-   SVM

Among the 5, logistic regression is the simplest. I will try to train all the models and see which one is the best to fit the data out.

#### Logistic Regression

The code:

```{r}
#| echo: true
logit_eng <- logistic_reg(penalty = "ROC")

logit_wf <- workflow() %>%
  add_model(logit_eng) %>%
  add_recipe(recipe_extract)

logit_mdl <- logit_wf %>%
  fit(data = train_set)

logit_res <- data.frame(passenger_id = test_set$passenger_id,
                        truth = as.factor(test_set$survived),
                        estimate = predict(logit_mdl, test_set)$.pred_class,
                        prob = predict(logit_mdl, test_set, type = "prob")$.pred_0) %>%
  left_join(recipe_extract %>% prep() %>% bake(test_set), by = "passenger_id") %>%
  mutate(cabin = test_set$cabin,
         age = test_set$age)
conf_mat(logit_res, truth = truth, estimate = estimate)
accuracy(logit_res, truth, estimate)
f_meas(logit_res, truth, estimate)
roc_curve(logit_res, truth, prob) %>% autoplot()
roc_auc(logit_res, truth, prob, event_level = "first")
```

```{r test_submission}
# pred_res <- data.frame(
#   PassengerId = val_set$passenger_id,
#   Survived = predict(logit_mdl, val_set)$.pred_class)
# # we get 0.77511 on the validation set
# write.csv(pred_res, here::here("logit_titanic_basic.csv"), row.names = FALSE)
```

#### Decision Tree

```{r}
# #| echo: true
# d_tree <- C5_rules()
# # 
# d_tree_wf <- workflow() %>%
#   add_model(d_tree) %>%
#   add_recipe(recipe_extract)
# 
# d_tree_mdl <- d_tree_wf %>%
#   fit(data = train_set)
# 
# d_tree_res <- data.frame(truth = as.factor(test_set$survived),
#                         estimate =predict(d_tree_mdl, test_set)$.pred_class, prob = predict(d_tree_mdl, test_set, type = "prob")$.pred_0)
# conf_mat(d_tree_res, truth = truth, estimate = estimate)
# accuracy(d_tree_res, truth, estimate)
# f_meas(d_tree_res, truth, estimate)
# roc_curve(d_tree_res, truth, prob) %>% autoplot()
# roc_auc(d_tree_res, truth, prob, event_level = "first")
```

#### Random Forest

```{r}
#| echo: true
r_forest <- rand_forest(mode = "classification")

r_forest_wf <- workflow() %>%
  add_model(r_forest) %>%
  add_recipe(recipe_extract)

r_forest_mdl <- r_forest_wf %>%
  fit(data = train_set)

r_forest_res <- data.frame(truth = as.factor(test_set$survived),
                        estimate =predict(r_forest_mdl, test_set)$.pred_class, prob = predict(r_forest_mdl, test_set, type = "prob")$.pred_0)
conf_mat(r_forest_res, truth = truth, estimate = estimate)
accuracy(r_forest_res, truth, estimate)
f_meas(r_forest_res, truth, estimate)
roc_curve(r_forest_res, truth, prob) %>% autoplot()
roc_auc(r_forest_res, truth, prob, event_level = "first")
```

```{r}
r_forest_pred_res <- data.frame(
  PassengerId = val_set$passenger_id,
  Survived = predict(r_forest_mdl, val_set)$.pred_class)
# we get 0.76076 on the validation set
write.csv(r_forest_pred_res, here::here("r_forest_ranger_basic.csv"), row.names = FALSE)
```

#### XGB

```{r}
my_recipe_xgb <- recipe_extract %>%
    step_dummy(all_nominal_predictors())
#| echo: true
xgb <- boost_tree(mode = "classification", engine = "xgboost")

xgb_wf <- workflow() %>%
  add_model(xgb) %>%
  add_recipe(my_recipe_xgb)

xgb_mdl <- xgb_wf %>%
  fit(data = train_set)

xgb_res <- data.frame(truth = as.factor(test_set$survived),
                        estimate =predict(xgb_mdl, test_set)$.pred_class, prob = predict(xgb_mdl, test_set, type = "prob")$.pred_0)
conf_mat(xgb_res, truth = truth, estimate = estimate)
accuracy(xgb_res, truth, estimate)
f_meas(xgb_res, truth, estimate)
roc_curve(xgb_res, truth, prob) %>% autoplot()
roc_auc(xgb_res, truth, prob, event_level = "first")
```

```{r}
xgb_pred_res <- data.frame(
  PassengerId = val_set$passenger_id,
  Survived = predict(xgb_mdl, val_set)$.pred_class)
# we get 0.74162 on the validation set
write.csv(xgb_pred_res, here::here("xgb_basic_add_ticket_num.csv"), row.names = FALSE)
```

#### GBM

```{r}
my_recipe_xgb <- recipe_extract %>%
    step_dummy(all_nominal_predictors())
#| echo: true
gbm <- boost_tree(mode = "classification", engine = "lightgbm")

gbm_wf <- workflow() %>%
  add_model(gbm) %>%
  add_recipe(my_recipe_xgb)

gbm_mdl <- gbm_wf %>%
  fit(data = train_set)

gbm_res <- data.frame(truth = as.factor(test_set$survived),
                        estimate =predict(gbm_mdl, test_set)$.pred_class, prob = predict(gbm_mdl, test_set, type = "prob")$.pred_0)
conf_mat(gbm_res, truth = truth, estimate = estimate)
accuracy(gbm_res, truth, estimate)
f_meas(gbm_res, truth, estimate)
roc_curve(gbm_res, truth, prob) %>% autoplot()
roc_auc(gbm_res, truth, prob, event_level = "first")
```

```{r}
recipe_extract %>% 
    prep() %>% 
    bake(test_set) %>% 
    mutate(predicted = gbm_res$estimate) %>% 
    filter(survived == 0, predicted == 1) %>% 
    select(pclass, sex, embarked, survived, ses_name)
```

#### SVM

```{r}
#| echo: true
svm <- svm_linear(mode = "classification", engine = "kernlab")

svm_wf <- workflow() %>%
  add_model(svm) %>%
  add_recipe(recipe_extract %>% 
                 step_dummy(all_nominal_predictors()))

svm_mdl <- svm_wf %>%
  fit(data = train_set)

svm_res <- data.frame(truth = as.factor(test_set$survived),
                        estimate = predict(svm_mdl, test_set)$.pred_class, prob = predict(gbm_mdl, test_set, type = "prob")$.pred_0)
paste("Confussion matrix SVM\n")
conf_mat(svm_res, truth = truth, estimate = estimate)
accuracy(svm_res, truth, estimate)
f_meas(svm_res, truth, estimate)
roc_curve(svm_res, truth, prob) %>% autoplot()
roc_auc(svm_res, truth, prob, event_level = "first")
```

```{r}
recipe_extract %>% 
    prep() %>% 
    bake(test_set) %>% 
    mutate(predicted = svm_res$estimate) %>% 
    filter(survived == 0, predicted == 1)
```

```{r}
svm_pred_res <- data.frame(
  PassengerId = val_set$passenger_id,
  Survived = predict(svm_mdl, val_set)$.pred_class)
# we get 0.77511 on the validation set
write.csv(svm_pred_res, here::here("svm_basic.csv"), row.names = FALSE)
```

### Model Evaluation

Based on the model I have trained previously, it seems
