---
title: "Titanic Life Prediction"
author: "Suberlin Sinaga"
header: |
  \usepackage[familydefault,regular]{Chivo} %% Option 'familydefault'
format:
  pdf:
    highlight-style: dracula
mainfont: "SourceSansPro-Regular"
editor_options: 
  chunk_output_type: console
---

```{r setup_chunk, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# importing library
suppressPackageStartupMessages({
    library(dplyr)
    library(tidymodels)
    library(tidyquant)
    library(C50)
    library(rules)
})

# Theming section
used_theme <- theme_tq(base_size = 12) +
    theme(plot.background = element_rect(fill = "#1A1A1A"),
          panel.background = element_rect(fill = "#1A1A1A"),
          strip.background = element_rect(fill = "#1A1A1A"),
          text = element_text(colour = "#e0e0e0", face = "bold"),
          axis.text = element_text(colour = "#e0e0e0"),
          strip.text = element_text(colour = "#e0e0e0"),
          legend.box.background = element_rect(fill = "#c3402a"),
          legend.background = element_rect(fill = "#c3402a"),
          legend.key = element_rect(fill = "#c3402a"),
          legend.position = "bottom",
          plot.caption = element_text(color = "#e0e0e0", size = 8),
          panel.grid.minor = element_blank(),
          panel.grid.major.x = element_blank(),
          panel.grid.major.y = element_line(colour = "#8C0000", linewidth = 0.2),
          plot.subtitle = element_text(size = 10),
          
    )


theme_set(used_theme)
# set up some function to avoid load the whole package
`%<>%` <- magrittr::`%<>%`
```


## DATA PREPARATION

### Data Import

The data comes from famous [kaggle titanic](https://www.kaggle.com/competitions/titanic) problem set.

```{r, echo=TRUE}
titanic_training_set <- read.csv(here::here("data/titanic/train.csv"))
titanic_test_set <- read.csv(here::here("data/titanic/test.csv"))
```

Since in this early dataset the training and testing set was separated, I will combine them to ensure that I explore all the data as a whole.

```{r, echo=TRUE}
titanic_raw_all <- rbind(
    titanic_test_set %>% mutate(Source = "test set", Survived = NA),
    titanic_training_set %>% mutate(Source = "train set")
)
```

Naturally, the testing set has no target variable, hence I set the target variable values into `NA` when the data came from testing set.

### Data Exploration

In this data exploration, I will simply see how each variable distributed and related to the target variable. Before zooming into each variable, I will see the whole variable first to get the big picture about the data.

```{r}
titanic_raw_all %>% str()
```

Here I have 13 variables, from PassengerId to Survived. The source variable is an addition to mark where the data comes from, so basically I have only 12 variables come from the data. First thing I notice here is that the name of the variable is not following `snake_case` format. Hence I will first standardized them.

```{r, echo=TRUE}
titanic_raw_all %<>% janitor::clean_names()

titanic_raw_all %>% str()
```

Now, the column names have followed the snake case format. The next step is to explore each variable to see how they distributed and related to the target variable.

#### 1. Survived Variable

This variable is basically a binary variable where the value is either 0 or 1 that represents whether the passenger will survive from the disaster or not. Consider the data, it should be stored as factor but yet the data was stored in `integer` data type.

Since basically this variable is not available in the test set, so naturally I will explore them from training set data only.

```{r}
titanic_training_set %>% 
    mutate(Survived = as.factor(Survived)) %>% 
    ggplot(aes(x = Survived, y = after_stat(count))) +
    geom_bar(width = 0.5) +
    geom_label(aes(label = after_stat(count)), stat = "count", vjust = 1)
```

The graph shows that people who survive on the training data is 342 and the one that is not survived is 549. The ratio between them is 1 : 1.6 which can be categorized as balance dataset.

#### 2. Passengerid Variable

This variable is basically just a random value assigned to the passenger. It purposes is to easily identify the passenger uniquely.

#### 3. Name Variable

In simple, name is something that everybody has. It is attached to a person but not unique for each person. I and you might have the same name. So basically, it just a random values attached to us to identify ourself and nothing to do with survival life. Naturally it will and should be deleted. But in this titanic class, name comes with a unique thing that furhter can be used to identify your socio economic class, such as Mr, Mrs, Jonkheer, etc.

```{r}
name_decoder <- function(x) {
    x = as.character(x)
    ses_name = sapply(x, function(y) { strsplit(y, ", ")[[1]][2]})
    identifier_name = sapply(ses_name, function(x) {strsplit(x, ".", fixed = TRUE)[[1]][1]})
    return(identifier_name)
}
titanic_raw_all %>% 
    mutate(names = name_decoder(name)) %>% 
    ggplot(aes(x = forcats::fct_infreq(names))) +
    geom_bar() +
    theme(axis.text.x = element_text(angle = 90))
```

The graph indicates that each names has the ses values. It is dominated by 4 big categories on overall data. I think it is safe to just lumped the rest as `other` categories instead.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(names = name_decoder(name),
           survived = as.factor(survived)) %>% 
    mutate(names = case_when(names %in% c("Mr", "Miss", "Mrs", "Master") ~ names, TRUE ~ "other")) %>% 
    ggplot(aes(x = forcats::fct_infreq(names))) +
    geom_bar(aes(fill = survived), position = "fill") +
    theme(axis.text.x = element_text(angle = 90))
```

As expected, the people with "Mr" has low survival rate compare to Miss and Mrs. This is natural since man was destined to protect women. As for "Master", they might bring their helper that willing to sacrifice their life to protect the master. As the graph says, this information not only telling me about gender but also about their survival life across social level.

#### 4. Sex variable

Previously from `name` variable, I can see both gender and socio-economic class have impact to the survival rate. I know that those people with prefix "Mr" will have low survival rate. Now, the question is, is it true that gender itself has a great impact to the survival rate?

```{r}
titanic_raw_all %>% 
  filter(!is.na(survived)) %>% 
  mutate(survived = as.factor(survived)) %>% 
  ggplot(aes(x = sex)) +
  geom_bar(aes(fill = survived), position = "fill")
```

In simple, the gender totally affects the survival rate. If you are a man, then you simply have less than 25% probability to survive. While, if you are woman you might have a probability to survive about 75%.

Finding this knowledge, now I would like to check if the previous SES from name has something to do with the gender.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(names = name_decoder(name),
           survived = as.factor(survived)) %>% 
    mutate(names = case_when(names %in% c("Mr", "Miss", "Mrs", "Master") ~ names, TRUE ~ "other")) %>% 
    ggplot(aes(x = forcats::fct_infreq(names))) +
    geom_bar(aes(fill = survived), position = "fill") +
    theme(axis.text.x = element_text(angle = 90)) +
    facet_wrap(.~sex, scales = "free_x")
```

The graph covers even more interesting information for me. On the "other" female, almost all the category is survived while for "other" male, it seems most of them are not survived. Compare to "Miss" category, the "Mrs" seems have higher survival rate. One of the reason that can explain is maybe they sacrificed their life for their children.

Another interesting thing is the master category on male gender, it has the highest survival rate among the other categories. As we know, this title is for children from a noble family. Hence, in my opinion this indicates that maybe instead of gender and SES, age also affects the survival rate.

#### 5. Pclass Variable

Pclass variable is a variable that describes what so called as socio-economic status. It is divided into 3 level, level 1 is for upper level, level 2 is for middle level and level 3 is for lower level. Considering the function of the field, it is best to be casted into ordinal factor instead of integer.

```{r}
titanic_raw_all %>% 
    mutate(pclass = factor(pclass, level = c(1,2,3))) %>% 
    ggplot(aes(x = pclass)) +
    geom_bar() +
    geom_label(aes(label = after_stat(count)), stat = "count", vjust = 1)
```

As the graph shown, that people with lowest tier of the ticket is much more than the people with high tier ticket type. Now, I wonder for each type of the ticket, is there any difference of survival life? Is the higher of the ticket class, the better the survival life will be?

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(pclass = factor(pclass, level = c(1,2,3)),
           survived = as.factor(survived)) %>% 
    ggplot(aes(x = pclass)) +
    geom_bar(aes(fill = survived), position = "fill")
```

The graph clearly indicated that the higher the ticket class, then the higher the survival life is. I think this graph itself is enough to state that the pclass variable is related to the survival rate.

Now, the next question for me, is there any difference between the pclass among the gender?

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(pclass = factor(pclass, level = c(1,2,3)),
           survived = as.factor(survived)) %>% 
    ggplot(aes(x = pclass)) +
    geom_bar(aes(fill = survived), position = "fill") +
    facet_wrap(. ~ sex)
```

What can I say from the graph is that, even across the gender the main rules of the pclass is still the same, the higher the class the better the survival rate.

#### 6. Age Variable

Age represents how old the passenger of the ship. In terms of survival rate, basically I would like to say that teenager might have higher survival rate than children or older people. But, before going further, I would like to check the distribution of the data first.

```{r}
titanic_raw_all %>% 
    ggplot(aes(x = age)) +
    geom_histogram()
```

The histogram of the age indicates that the passenger's age is ranging from ~0 to 80 years old. The warning message indicates that there are about 263 missing values on age variable. Now, the question is then how age is currently related to survival rate? Is it true that teenager might have higher survival rate?

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    ggplot(aes(x = age)) +
    geom_histogram(aes(fill = as.factor(survived)))
```

Very interesting. Basically we can say that for survival condition, the lesser the age, then the higher the probability to survive. It is shown in the plot on survived condition, where the older age survival height dropped so much while for the lesser age not getting that deep. Take a look the following chart.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    ggplot(aes(x = age)) +
    geom_histogram(aes(fill = as.factor(survived))) +
    annotate("rect", xmin = 14, xmax = 56, ymin = 0, ymax = 80, col = "#ffdabd",
             alpha = 0) +
    annotate("text", x = 40, y = 74, label = "The survival rate for age \nbetween 15 and 56 dropped so deep.", col = "#ffdabd", size = 2) +
    annotate("rect", xmin = 0, xmax = 8, ymin = 0, ymax = 40, col = "green",
             alpha = 0) +
    annotate("text", x = 4, y = 47, label = "The drop not that big for\nage between 0-8 years", col = "green", size = 2) +
    labs(fill = "Survived",
         title = "Age Distribution Among the Survival Condition",
         x = "Passenger's Age",
         y = "Count",
         caption = "@2023 Suberlin Sinaga")
```

The next question comes to my head is then, how age survival distributed across gender? Did this condition happen for all gender, or there is another condition on gender level?

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    ggplot(aes(x = age)) +
    geom_histogram(aes(fill = as.factor(survived))) +
    facet_wrap(. ~ sex)
```

Wonderful findings. In fact, the dropping survival rate for the older age is for men only. For the women the drop between older and younger age are slightly equal. Take a look at the following plot to prove what I said.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(age = case_when(between(round(age), 0, 8) ~ "1. 0-8",
                           between(round(age), 9, 20) ~ "2. 9-20",
                           between(round(age), 21, 40) ~ "3. 21-40",
                           between(round(age), 41, 60) ~ "4. 41-60",
                           TRUE ~ "5. >60"), survived = as.factor(survived)) %>% 
    count(age, survived, sex) %>% 
    group_by(age, sex) %>% 
    mutate(surv_rate = n/sum(n)) %>% 
    ggplot() + 
    geom_line(aes(x = age, y = surv_rate, col = survived, group = survived)) + 
    geom_point(aes(x = age, y = surv_rate, col = survived, size = n)) + 
    facet_wrap(sex ~ .)
```

In th plot, color indicates the survival condition, while the size of point determine the number of people. The plot supports my previous statement when the survival condition is very different between the older and younger age. Considering this, I think I will set the bracker category for the age.

#### 7. Sib_sp Variable

Sibsp variable is talking about how many siblings and or spouse brought into the ship.

```{r}
titanic_raw_all %>%
    filter(!is.na(survived)) %>% 
    mutate(survived = as.factor(survived)) %>% 
    ggplot(aes(x = sib_sp)) +
    geom_bar(aes(fill = survived), position = "fill") +
    scale_x_continuous(breaks = 0:8)
```

It seems that your probability of survive will greatly increase if you have only 1 siblings. The probability seems to drop either you don't have siblings or spouse or you have more siblings or spouse. Now, the question is just like previously, is this condition apply to all gender?

```{r}
titanic_raw_all %>%
    filter(!is.na(survived)) %>% 
    mutate(survived = as.factor(survived)) %>% 
    ggplot(aes(x = sib_sp)) +
    geom_bar(aes(fill = survived), position = "fill") +
    scale_x_continuous(breaks = 0:8) +
    facet_wrap(. ~ sex, scales = "free_x")
```

I think different condition happens when the data was splited into gender. For the women, their survival rate will only get dropped when they have more than 2 siblings or spouse. While for men, they will only have slight higher survival probability when they have 1 sibling or spouse.

#### 8. Parch Variable

Parch variable talks about parents and children onboarded in the ship. In fact, if you are 5 years old, you might get benefit if you get onboarded with your parents since they will save you first. But in fact, if you are a grown man, you are the one who should save your parents. Considering this scheme, I think it is worth to check how this variable is related to the survival rate.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(survived = as.factor(survived)) %>%
    ggplot(aes(x = parch)) +
    geom_bar(aes(fill = survived), position = "fill") +
    scale_x_continuous(breaks = 0:8)
```

Having parents and or children could mean having more responsibility for their life. But the responsibility is not that simple. A women might be the priority for his parents to be saved first. For a men, it is a honor for him to sacrifice his life. As a parents it will be a happiness to see their 10 years old boy alive in the future, and so on. So having parents and or children might need another variable to describe their effect to the target variable. One of the variable to help explaining them is gender.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    mutate(survived = as.factor(survived)) %>%
    ggplot(aes(x = parch)) +
    geom_bar(aes(fill = survived), position = "fill") +
    scale_x_continuous(breaks = 0:8) +
    facet_wrap(. ~ sex, scales = "free_x")
```

As the plot suggests, for a man, the more you have parents/children the less probability you will survive. It seems unlikely for a women, the more you have children/parents the higher probability you will survive.

In fact, I think this variable can not be identify alone since we have another equal variable, the sibling or spouse. I just wonder, how the effect of the big family when they are included into one single variable.

```{r}
titanic_raw_all %>% 
    mutate(family_num = parch + sib_sp,
           survived = as.factor(survived)) %>%
    filter(!is.na(survived)) %>% 
    ggplot(aes(x = family_num)) +
    geom_bar(aes(fill = survived)) +
    scale_x_continuous(breaks = 0:10) +
    facet_wrap(. ~ sex, scales = "free_x")
```

Consider this plot, what I see is that for each family number category, the survival rate is different across the category. But for people with greater than 2 family number, there are a very small values to explain the variance in the data. Hence, I think, I will cap the values into 3 max. So if someone has more than 3 family, they will be considered as having 3 family number.

```{r}
titanic_raw_all %>% 
    mutate(family_num = if_else((parch + sib_sp) >= 3, 3L,(parch + sib_sp)),
           survived = as.factor(survived)) %>%
    filter(!is.na(survived)) %>% 
    ggplot(aes(x = family_num)) +
    geom_bar(aes(fill = survived), position = "fill") +
    scale_x_continuous(breaks = 0:10) +
    facet_wrap(. ~ sex, scales = "free_x")
```

Now I think, the data is more representative to tell the story behind the survival rate.

#### 9. Ticket Variable

The ticket variable contains a random variable that most likely has nothing to do with the survival rate. Hence, I think I just need to drop it.

#### 10. Fare Variable

Fare variable talks about the fare that the passenger should pay to get onboarded into the ship.

```{r}
titanic_raw_all %>%
    ggplot(aes(x = fare)) +
    geom_histogram()
```

The histogram of the fare said that most of the fare is less than 100. The data has outlier for the fare about 500.

```{r}
titanic_raw_all %>% 
    filter(fare > 500, !is.na(survived))
```

Here I can see that people who paid for the fare greater than 500 is 100% survive. Considering the number of the data, this is might due to coincidence. To tell the data better, I think, I will separate the data into two categories, the first one is greater than or equal to 50 and less than 50.

```{r}
titanic_raw_all %>%
    filter(!is.na(survived)) %>% 
    mutate(fare_category = case_when(fare < 50 ~ "< 50", TRUE ~ ">= 50")) %>% 
    ggplot(aes(x = fare_category, fill = as.factor(survived))) +
    geom_bar(position = "fill") +
    facet_wrap(. ~ sex)
```

I think for both situation, either for men or women, the higher fare has higher survival rate. This means that the fare has affect to the survival rate even across gender.

#### 11. Cabin

The cabin variable mostly talk about the cabin name/code where the passengers placed. But somehow, there are so many NAs in this data.

```{r}
titanic_raw_all %>% 
    mutate(cabin = case_when(cabin == "" ~ NA, TRUE ~ "has cabin")) %>% 
    count(cabin)
```

There is about 1014 or 77% the data is missing. This is of course hard to interpret or guess the missing values. Hence, I think I will drop the variable.

#### 12. Embarked

Embarked variable is a variable that talks about the port where the passengers are embarked from.

```{r}
titanic_raw_all %>% 
    ggplot(aes(x = forcats::fct_infreq(embarked))) +
    geom_bar() +
    geom_label(aes(label = after_stat(count)), stat = "count", vjust = 0)
```

From the whole data set, I found out that 2 out of them are missing. Now, the main question left behind is, how embarked variable explains the survival rate.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    ggplot(aes(x = forcats::fct_infreq(embarked))) +
    geom_bar(aes(fill = as.factor(survived)), position = "fill")
```

The plot clearly indicates that there is slightly different results for different embarked. In order to have better inference, I think I will zoom the data in to gender.

```{r}
titanic_raw_all %>% 
    filter(!is.na(survived)) %>% 
    ggplot(aes(x = forcats::fct_infreq(embarked))) +
    geom_bar(aes(fill = as.factor(survived)), position = "fill") +
    facet_wrap(sex ~ .)
```

I think, for each embarked category, the gender plays a little effect here. It only affects people embarked from Q port. The men embarked from Q port has is the lowest survival rate for men, while it doesn't apply for women.

Considering this, I think this variable can be used to add prediction power for the survival list.

**Conclusion**
==========

After doing some data exploration, there are some conclusions that I can draw.

1. Passenger ID and ticket is just a random variables that bring no effect to the data. These variables should be just dropped without any further exploration.

2. Name variable is another random variable, but it contains what is known as SES data such as Mr, Mrs, Miss, etc. So the name variable will only be able to **be deleted** after the feature extraction done.

3. Cabin variables are just have so many missing values and it is impossible to extract any information from it, so I will just drop it out.

4. Socio economic status from name, sex, pclass, age, family number, fare and embarked variables have significant effect tot he survival rate.

5. Survived, pclass, embarked, name, and sex variables have assigned to wrong data type. Those are more approriate to be stored as factor.

6. Variables like embarked, age and fare have missing values that have to deal with.

7. New variable such as number of family seems to have more significant effect to the survival rate compare to sibling and spouse, and parent and children alone.

8. Some categories such as the SES categories extracted from name variable might need to lumped into fewer to avoid overfitting due to low data points.

### Data Splitting

For this titanic use case, I will split the data into 3 types of data. The first one is validation test which is the `test` set coming from the competition where the result will be submitted. The `train` set data from the competition will be separated into `train` set and `test` set.

```{r echo=TRUE}
validation_set <- titanic_raw_all %>% 
    filter(source == "test set") %>% 
    select(-source)

i_train <- initial_split(titanic_raw_all %>% filter(source == "train set") %>% select(-source), prop = 3/4)

train_set <- training(i_train)
    
test_set <- testing(i_train)
```

## DATA MODELING

### Data Preprocessing

In this preprocessing data, I will use 4C frameworks (correcting, completing, creating, converting) and tidy principal. I will create a recipe where survived is the outcome. Variable of sex, age, fare, embarked, pclass, sib_sp and parch will be predictor variables by default.

```{r}
my_recipe <- recipe(train_set) %>% 
    update_role(survived, new_role = "outcome") %>% 
    update_role(sex, age, fare, embarked, pclass, sib_sp, parch, new_role = "predictor") %>% 
    step_string2factor(all_nominal_predictors())
```


#### Correcting

I didn't find any field that seems need to be corrected in the data. So I will skip this step.

#### Completing

```{r}
titanic_raw_all %>% 
    select(-c(source,survived)) %>% 
    DataExplorer::plot_missing(missing_only = TRUE, theme_config = used_theme)
```

Previously, there were 3 variables have missing values. But now, the plot stated that there is only 2 variables. The reason behind this is that the missing values on embarked variable is not missing values, instead it is an empty string (""). Hence, I need to convert it first to missing values in order to ensure that it is acknowledged as missing values.

```{r}
titanic_raw_all %>% 
    select(-c(source,survived)) %>% 
    mutate(embarked = if_else(embarked == "", NA, embarked)) %>% 
    DataExplorer::plot_missing(missing_only = TRUE, theme_config = used_theme)
```

Now, the question is how I will deal with the missing values? Dealing with the missing values can be a bit tricky. Mostly it depends on data type and data distribution. When the data type is categorical, usually the mode is used to fill out the missing values. When the data type is numerical variable, it will depend on the distribution of the data.

Based on this understanding, the embarked variable will be filled using mode. I think for the age variable can be filled using mean, and the fare variable can be filled using median.

```{r filling_missing_values, echo=TRUE}
my_recipe %<>% 
    step_mutate(embarked = if_else(embarked == "", NA, embarked)) %<>% 
    step_impute_mean(age) %<>% 
    step_impute_mode(embarked) %<>% 
    step_mutate(embarked = droplevels(embarked)) %<>% 
    step_impute_median(fare)
```

#### Creating

The new variable I want to create is family number by adding parent, children, sibling, and spouse all together.

```{r}
#| echo: true
my_recipe %<>% 
    step_mutate(family_num = parch + sib_sp, role = "predictor")
```

#### Converting

Converting is meant to convert the data type into what it should be. Previously, I have found out that some variables need to be converted into certain types. For example, pclass needs to be converted into ordinal factor since it is stored as integer. Surived, embarked, name and sex variables should also be stored as factor.

```{r}
#| echo: true
my_recipe %<>% 
  step_num2factor(pclass, levels = c("1","2","3"), ordered = TRUE) %<>% 
  step_mutate(survived = factor(survived, levels = c("0", "1")), skip = TRUE)
```

### Feature Engineering

#### Feature Extraction

The feature extraction is used when there is a useful feature in another feature regardless the feature is important or not.

```{r}
my_recipe %<>% 
  step_mutate(ses_name = case_when(name_decoder(name) %in% c("Mr", "Miss", "Mrs", "Master") ~ name_decoder(name), TRUE ~ "Other") %>% factor(levels = c("Mr", "Miss", "Mrs", "Master", "Other")), role = "predictor")
```

#### Feature Selection

In machine learning model, there is a Jargon known as **"garbage in, garbage out"**. It means, if we feed our model with garbage (irrelevant and un-useful features) then we will get another garbage as the output. Hence, feature selection is very important.

Based on my previous exploration, each variables have different affect on the target variable.

* **Sex variable** is the most important variable to predict the survival rate. In simple, by saying that the men are all die and the women are all survive we will have ~75% accuracy of the model.

* **Name variable** is not directly related to the survival rate. But it contains Socio Economic Status and gender related that can explain the survival rate even further. For example, even if you are a man but you are a `Master` then you will have greater than 50% survival rate compare to the common men. While for women, you will have a bit lower survival rate if you are a `Miss`. This indicates that the SES in a name can epxlain the variability even further.

* **Pclass variable** is another SES variable. Regardless the gender, you will have higher survival rate if you have higher class.

* **Age variable** describes how old the passenger is. In terms of survival rate, it seems that your survival rate will greatly dropped if you are a men with age between 18 and 56. It seems that age has not much effect to women.

* **Sib_sp variable** talks about sibling and spouse get onboarded into the ship. In general, it was stated that the more family you have, the less your survival rate will be.

* **Parch variable** explains about parent and children you brought into the ship. It is found that if you have more parents and or children up until 3, your survival rate will be higher, but it will be dropped otherwise.

* **Family_num variable** was created by adding the number of sibling/spouse and parent/children you brought into the ship. It seems that the effect is different across gender. For women, the more you bring your family, the less your survival rate will be, otherwise for men.

* **Fare variable** talks about the value of the ticket paid by the passengers. The higher the price paid, the higher the probability to survive.

* **Embarked variable** explains about from where the passengers were departed from. Across the gender, the C port has the highest probability to survive.

From above explanations, I think all those variables can explain the variability of the data. Hence I think I can use them to train the model.

Feature Importance
====

Feature importance is one of feature selection method that using a model to asses how important a feature to the model. Assuming all the variables above were getting used. I will measure how important each variable to predict the target variable. I will use simple logistic regression to build the measurement.

There are several types of variable importance measurement that can be done in R, they are permutation based, variance based and shapley based. While for the metrics, we can use accuracy, f1 score, roc_auc, and so on.

```{r}
predict_kern <- function(object, newdata = train) {kernlab::predict(object, newdata) %>% mutate_all(as.numeric) %>% .[[".pred_class"]]}
predict_kern_frame <- function(object, newdata = train) {kernlab::predict(object, newdata) %>% mutate_all(as.numeric) }
my_recipe %<>% 
  step_rm(name, ticket, passenger_id, cabin)
logit_engine <- logistic_reg(penalty = "roc_auc")

logit_wf <- workflow() %>% 
  add_model(logit_eng) %>% 
  add_recipe(my_recipe)

logit_mdl <- logit_wf %>% 
  fit(data = train_set)

logit_mdl %>% 
    extract_fit_parsnip() %>% 
    vi(
        method = "firm",
        train = juice(my_recipe %>% prep()) %>% mutate(survived = survived),
        target = "survived",
        metric = "roc_auc",
        pred_wrapper = predict_kern,
        event_level = "first",
        feature_names = c("sex","embarked","pclass","sib_sp","parch","ses_name","age","family_num","fare")
    ) %>% 
  ggplot(aes(x = reorder(Variable, desc(Importance)), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  # ggtitle(bquote(FeatureImportance^2))
  labs(title = expression(paste("Feature Importance using ", FIRM^1," Method")))

logit_mdl %>% 
    extract_fit_parsnip() %>% 
    vi(
        method = "permute",
        train = juice(my_recipe %>% prep()) %>% mutate(survived = survived),
        target = "survived",
        metric = "roc_auc",
        pred_wrapper = predict_kern,
        # pred_wrapper = kernlab::pred,
        # event_level = "first",
        feature_names = c("sex","embarked","pclass","sib_sp","parch","ses_name","age","family_num","fare")
    ) %>% 
  ggplot(aes(x = reorder(Variable, desc(Importance)), y = Importance)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = expression(paste("Feature Importance using ", PERMUTE^2," Method")))
```

As expected, the sex and ses_name variables are the most important variables on both measurement. Another thing for sure, `family_num` have no importance for both measurements. This is expected. Since family_num is the total of `sib_sp` and `parch` is the addition between the two, then they will naturally be colinear to each other.

#### Feature Transformation

In this case, I think there is no variable that needs to be transformed. So i will skip this process.

### Model Training & Evaluation

Previously, I have built the data understanding for each variables. Now, I will model the data to predict the survival rate. There are currently 9 variables that I will use as features. The question is, what kind of algorithm that should be used to train the data. There are several types of classification algorithm.

* Logistic regression

* Decision Tree

* Random Forest

* XGB

* GBM

* SVM

Among the 5, logistic regression is the simplest. I will try to train all the models and see which one is the best to fit the data out.

#### Logistic Regression

The code:

```{r}
#| echo: true
logit_eng <- logistic_reg(penalty = "ROC")

logit_wf <- workflow() %>% 
  add_model(logit_eng) %>% 
  add_recipe(my_recipe %>% step_rm(family_num))

logit_mdl <- logit_wf %>% 
  fit(data = train_set)

logit_res <- data.frame(truth = as.factor(test_set$survived), estimate = predict(logit_mdl, test_set)$.pred_class, prob = predict(logit_mdl, test_set, type = "prob")$.pred_0)
conf_mat(logit_res, truth = truth, estimate = estimate)
accuracy(logit_res, truth, estimate)
f_meas(logit_res, truth, estimate)
roc_curve(logit_res, truth, prob) %>% autoplot()
 roc_auc(logit_res, truth, prob, event_level = "first")
```

```{r test_submission}
pred_res <- data.frame(
  PassengerId = validation_set$passenger_id,
  Survived = predict(logit_mdl, validation_set)$.pred_class)
# we get 0.77511 on the validation set
write.csv(pred_res, here::here("logit_titanic_basic.csv"), row.names = FALSE)
```

#### Decision Tree

```{r}
#| echo: true
d_tree <- C5_rules()

d_tree_wf <- workflow() %>% 
  add_model(d_tree) %>% 
  add_recipe(my_recipe)

d_tree_mdl <- d_tree_wf %>% 
  fit(data = train_set)

d_tree_res <- data.frame(truth = as.factor(test_set$survived),
                        estimate =predict(d_tree_mdl, test_set)$.pred_class, prob = predict(d_tree_mdl, test_set, type = "prob")$.pred_0)
conf_mat(d_tree_res, truth = truth, estimate = estimate)
accuracy(d_tree_res, truth, estimate)
f_meas(d_tree_res, truth, estimate)
roc_curve(d_tree_res, truth, prob) %>% autoplot()
roc_auc(d_tree_res, truth, prob, event_level = "first")
```


```{r}
pred_res <- data.frame(
  PassengerId = validation_set$passenger_id,
  Survived = predict(d_tree_mdl, validation_set)$.pred_class)
# we get 0.75837 on the validation set
write.csv(pred_res, here::here("logit_dtree_basic.csv"), row.names = FALSE)
```

#### Random Forest

```{r}
#| echo: true
r_forest <- rand_forest(mode = "classification")

r_forest_wf <- workflow() %>% 
  add_model(r_forest) %>% 
  add_recipe(my_recipe)

r_forest_mdl <- r_forest_wf %>% 
  fit(data = train_set)

r_forest_res <- data.frame(truth = as.factor(test_set$survived),
                        estimate =predict(r_forest_mdl, test_set)$.pred_class, prob = predict(r_forest_mdl, test_set, type = "prob")$.pred_0)
conf_mat(r_forest_res, truth = truth, estimate = estimate)
accuracy(r_forest_res, truth, estimate)
f_meas(r_forest_res, truth, estimate)
roc_curve(r_forest_res, truth, prob) %>% autoplot()
roc_auc(r_forest_res, truth, prob, event_level = "first")
```


```{r}
r_forest_pred_res <- data.frame(
  PassengerId = validation_set$passenger_id,
  Survived = predict(r_forest_mdl, validation_set)$.pred_class)
# we get 0.76076 on the validation set
write.csv(r_forest_pred_res, here::here("r_forest_ranger_basic.csv"), row.names = FALSE)
```

#### XGB

```{r}
my_recipe_xgb <- my_recipe %>% 
    step_dummy(all_nominal_predictors())
#| echo: true
xgb <- boost_tree(mode = "classification", engine = "xgboost")

xgb_wf <- workflow() %>% 
  add_model(xgb) %>% 
  add_recipe(my_recipe_xgb)

xgb_mdl <- xgb_wf %>% 
  fit(data = train_set)

xgb_res <- data.frame(truth = as.factor(test_set$survived),
                        estimate =predict(xgb_mdl, test_set)$.pred_class, prob = predict(xgb_mdl, test_set, type = "prob")$.pred_0)
conf_mat(xgb_res, truth = truth, estimate = estimate)
accuracy(xgb_res, truth, estimate)
f_meas(xgb_res, truth, estimate)
roc_curve(xgb_res, truth, prob) %>% autoplot()
roc_auc(xgb_res, truth, prob, event_level = "first")
```


```{r}
xgb_pred_res <- data.frame(
  PassengerId = validation_set$passenger_id,
  Survived = predict(xgb_mdl, validation_set)$.pred_class)
# we get 0.74162 on the validation set
write.csv(xgb_pred_res, here::here("xgb_basic.csv"), row.names = FALSE)
```

#### GBM

```{r}
my_recipe_xgb <- my_recipe %>% 
    step_dummy(all_nominal_predictors())
#| echo: true
gbm <- boost_tree(mode = "classification", engine = "lightgbm")

gbm_wf <- workflow() %>% 
  add_model(gbm) %>% 
  add_recipe(my_recipe)

gbm_mdl <- gbm_wf %>% 
  fit(data = train_set)

gbm_res <- data.frame(truth = as.factor(test_set$survived),
                        estimate =predict(gbm_mdl, test_set)$.pred_class, prob = predict(gbm_mdl, test_set, type = "prob")$.pred_0)
conf_mat(gbm_res, truth = truth, estimate = estimate)
accuracy(gbm_res, truth, estimate)
f_meas(gbm_res, truth, estimate)
roc_curve(gbm_res, truth, prob) %>% autoplot()
roc_auc(gbm_res, truth, prob, event_level = "first")
```


```{r}
gbm_pred_res <- data.frame(
  PassengerId = validation_set$passenger_id,
  Survived = predict(gbm_mdl, validation_set)$.pred_class)
# we get 0.74162 on the validation set
write.csv(gbm_pred_res, here::here("gbm_basic.csv"), row.names = FALSE)
```

#### SVM

```{r}
my_recipe_xgb <- my_recipe %>% 
    step_dummy(all_nominal_predictors())
#| echo: true
svm <- svm_linear(mode = "classification", engine = "kernlab")

svm_wf <- workflow() %>% 
  add_model(svm) %>% 
  add_recipe(my_recipe %>% step_rm(parch))

svm_mdl <- svm_wf %>% 
  fit(data = train_set)

svm_res <- data.frame(truth = as.factor(test_set$survived),
                        estimate =predict(svm_mdl, test_set)$.pred_class, prob = predict(gbm_mdl, test_set, type = "prob")$.pred_0)
paste("Confussion matrix SVM\n")
conf_mat(svm_res, truth = truth, estimate = estimate)
accuracy(svm_res, truth, estimate)
f_meas(svm_res, truth, estimate)
roc_curve(svm_res, truth, prob) %>% autoplot()
roc_auc(svm_res, truth, prob, event_level = "first")
```


```{r}
svm_pred_res <- data.frame(
  PassengerId = validation_set$passenger_id,
  Survived = predict(svm_mdl, validation_set)$.pred_class)
# we get 0.77511 on the validation set
write.csv(svm_pred_res, here::here("svm_basic.csv"), row.names = FALSE)
```

### Model Evaluation

Based on the model I have trained previously, it seems 